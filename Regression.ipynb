{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "->Linear regression is a fundamental statistical method used to model the relationship between a dependent variable and one or more independent variables. It assumes a linear relationship between the variables, meaning the dependent variable changes proportionally with changes in the independent variablesLinear regression is a fundamental statistical method used to model the relationship between a dependent variable and one or more independent variables. It assumes a linear relationship between the variables, meaning the dependent variable changes proportionally with changes in the independent variables\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "->The key assumptions of Simple Linear Regression are:\n",
        "\n",
        "Linearity: The relationship between the independent and dependent variables is linear.\n",
        "Independence: The observations are independent of each other.\n",
        "Homoscedasticity: The variance of the errors (residuals) is constant across all levels of the independent variable.\n",
        "Normality: The errors (residuals) are normally distributed.\n",
        "No Multicollinearity: (This is more relevant for Multiple Linear Regression, but it's good to mention that in simple linear regression, there's only one independent variable so multicollinearity is not an issue).\n",
        "\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y=mX+c?\n",
        "->In the equation Y = mX + c, which represents a simple linear regression model:\n",
        "\n",
        "The coefficient m represents the slope of the regression line. It indicates the change in the dependent variable (Y) for a one-unit increase in the independent variable (X), assuming all other factors are held constant. In other words, it quantifies the steepness and direction of the linear relationship between X and Y.\n",
        "\n",
        "4. What does the intercept c represent in the equation Y=mx+c?\n",
        "->In the equation Y = mX + c, which represents a simple linear regression model:\n",
        "\n",
        "The intercept c represents the value of the dependent variable (Y) when the independent variable (X) is zero. It is the point where the regression line crosses the Y-axis. In some cases, the intercept might not have a meaningful interpretation in the context of the data if a value of zero for the independent variable is outside the range of the observed data or is not a realistic scenario.\n",
        "\n",
        "\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "->In Simple Linear Regression, the slope m (also often represented as $\\beta_1$$\\beta_1$) in the equation $Y = mX + c$$Y = mX + c$ (or $Y = \\beta_0 + \\beta_1 X$$Y = \\beta_0 + \\beta_1 X$) is calculated using the following formula:\n",
        "\n",
        "$m = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}$$m = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}$\n",
        "\n",
        "Where:\n",
        "\n",
        "$x_i$$x_i$ and $y_i$$y_i$ are the individual data points for the independent and dependent variables, respectively.\n",
        "$\\bar{x}$$\\bar{x}$ is the mean of the independent variable.\n",
        "$\\bar{y}$$\\bar{y}$ is the mean of the dependent variable.\n",
        "$n$$n$ is the number of data points.\n",
        "This formula essentially represents the covariance of X and Y divided by the variance of X. It finds the value of m that minimizes the sum of the squared differences between the observed Y values and the predicted Y values (the least squares method).\n",
        "\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "->The least squares method in Simple Linear Regression is used to find the best-fitting straight line through a set of data points. The \"best-fitting\" line is defined as the one that minimizes the sum of the squared vertical distances (or residuals) between the observed data points and the line.\n",
        "\n",
        "In essence, it works by finding the values for the slope (m) and the intercept (c) in the equation $Y = mX + c$$Y = mX + c$ that result in the smallest possible sum of the squared differences between the actual dependent variable values ($Y$$Y$) and the values predicted by the linear model ($\\hat{Y}$$\\hat{Y}$).\n",
        "\n",
        "By minimizing the sum of squared residuals, the least squares method provides a way to determine the linear relationship that best represents the overall trend in the data, making it a fundamental technique for estimating the parameters of a linear regression model.\n",
        "\n",
        "7. How is the coefficient of determination (R2) interpreted in Simple Linear Regression?\n",
        "->The coefficient of determination (R²) in Simple Linear Regression is a statistical measure that represents the proportion of the variance in the dependent variable that is predictable from the independent variable.\n",
        "\n",
        "In simpler terms, it tells you how well the regression model fits the observed data.\n",
        "\n",
        "R² values range from 0 to 1.\n",
        "An R² of 0 means that the independent variable does not explain any of the variation in the dependent variable.\n",
        "An R² of 1 means that the independent variable explains all of the variation in the dependent variable.\n",
        "An R² value between 0 and 1 indicates the proportion of the dependent variable's variance that is explained by the independent variable. For example, an R² of 0.75 means that 75% of the variation in the dependent variable can be explained by the independent variable in the model.\n",
        "It's important to note that a high R² does not necessarily mean the model is good or that the relationship is causal. It only indicates the strength of the linear relationship and how well the model fits the data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "8. What is Multiple Linear Regression?\n",
        "->Multiple Linear Regression is an extension of simple linear regression. While simple linear regression models the relationship between a dependent variable and a single independent variable, multiple linear regression models the relationship between a dependent variable and two or more independent variables.\n",
        "\n",
        "The goal is still to find a linear equation that best predicts the value of the dependent variable based on the values of the multiple independent variables. The equation for multiple linear regression with 'p' independent variables is typically written as:\n",
        "\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "->The main difference between Simple Linear Regression and Multiple Linear Regression lies in the number of independent variables used to predict the dependent variable:\n",
        "\n",
        "Simple Linear Regression: Uses one independent variable to model the linear relationship with the dependent variable. The equation is typically $Y = mX + c$$Y = mX + c$ or $Y = \\beta_0 + \\beta_1 X$$Y = \\beta_0 + \\beta_1 X$.\n",
        "Multiple Linear Regression: Uses two or more independent variables to model the linear relationship with the dependent variable. The equation is typically $Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p + \\epsilon$$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p + \\epsilon$.\n",
        "In essence, Simple Linear Regression is a special case of Multiple Linear Regression where there is only one independent variable. Multiple Linear Regression allows for more complex modeling by considering the influence of multiple factors simultaneously.\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "->The key assumptions of Multiple Linear Regression are similar to those of Simple Linear Regression, with some extensions due to the presence of multiple independent variables. Here are the main assumptions:\n",
        "\n",
        "Linearity: The relationship between the dependent variable and each independent variable is linear, and the independent variables are linearly related to the dependent variable in combination.\n",
        "Independence of Residuals: The residuals (the differences between the observed and predicted values) are independent of each other. This means there is no correlation between consecutive residuals or between residuals from different observations.\n",
        "Homoscedasticity: The variance of the residuals is constant across all levels of the independent variables. This means the spread of the residuals is roughly the same for all predicted values of the dependent variable.\n",
        "Normality of Residuals: The residuals are normally distributed. While the dependent variable itself doesn't need to be normally distributed, the distribution of the errors should approximate a normal distribution, especially for smaller sample sizes.\n",
        "No Multicollinearity: The independent variables are not highly correlated with each other. High multicollinearity can make it difficult to interpret the individual coefficients and can lead to unstable estimates.\n",
        "\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "->Heteroscedasticity refers to the situation where the variance of the errors (residuals) in a regression model is not constant across all levels of the independent variables. In simpler terms, the spread of the residuals is not the same for all predicted values of the dependent variable.\n",
        "\n",
        "How it affects Multiple Linear Regression:\n",
        "\n",
        "Inefficient coefficient estimates: While the coefficient estimates remain unbiased in the presence of heteroscedasticity, they are no longer the most efficient. This means that the standard errors of the coefficients will be incorrect, leading to unreliable hypothesis tests and confidence intervals.\n",
        "Incorrect standard errors: The standard errors of the regression coefficients will be biased, usually underestimated. This can lead to incorrect conclusions about the statistical significance of the independent variables. You might conclude that a variable is statistically significant when it is not, or vice versa.\n",
        "Unreliable hypothesis tests and confidence intervals: Because the standard errors are incorrect, any hypothesis tests (like t-tests for individual coefficients) and confidence intervals based on these standard errors will be unreliable.\n",
        "In summary, heteroscedasticity doesn't bias the coefficient estimates themselves, but it makes all the inferences about those coefficients (significance, confidence intervals) unreliable. It's important to detect and address heteroscedasticity to ensure the validity of your regression results.\n",
        "\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "->You've identified a common challenge in Multiple Linear Regression. High multicollinearity occurs when two or more independent variables are highly correlated with each other. This can cause problems in the model, including:\n",
        "\n",
        "Unstable coefficient estimates: The coefficients of the correlated variables can be highly sensitive to small changes in the data, making them difficult to interpret.\n",
        "Inflated standard errors: Multicollinearity increases the standard errors of the coefficients, making it harder to determine which independent variables are statistically significant.\n",
        "Here are some common techniques to improve a Multiple Linear Regression model with high multicollinearity:\n",
        "\n",
        "Remove one of the correlated variables: If two or more variables are highly correlated, you can often remove one of them without losing much information. This is often the simplest solution.\n",
        "Combine correlated variables: You can create a new variable that is a combination of the correlated variables. For example, if you have variables for height and weight, you could create a new variable for Body Mass Index (BMI).\n",
        "Use dimensionality reduction techniques: Techniques like Principal Component Analysis (PCA) can be used to reduce the number of independent variables while retaining most of the information. PCA creates new uncorrelated variables (principal components) that are linear combinations of the original variables.\n",
        "\n",
        "\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "->Transforming categorical variables is essential before including them in most regression models, as these models typically require numerical input. Here are some common techniques:\n",
        "\n",
        "1. One-Hot Encoding:\n",
        "How it works: This is one of the most widely used techniques. For each categorical variable, it creates new binary (dummy) variables, one for each unique category. If an observation belongs to a specific category, the corresponding dummy variable is set to 1, and all other dummy variables for that original variable are set to 0.\n",
        "Example: If you have a \"Color\" variable with categories \"Red,\" \"Blue,\" and \"Green,\" one-hot encoding would create three new variables: \"Color_Red,\" \"Color_Blue,\" and \"Color_Green.\" An observation with \"Red\" would have \"Color_Red\" = 1, \"Color_Blue\" = 0, and \"Color_Green\" = 0.\n",
        "Consideration: This can lead to a large number of new variables if a categorical variable has many unique categories (the \"dummy variable trap\"). To avoid perfect multicollinearity, you typically drop one of the dummy variables for each original categorical variable.\n",
        "2. Label Encoding (Ordinal Encoding):\n",
        "How it works: This technique assigns a unique integer to each category.\n",
        "Example: For the \"Color\" variable, you might assign \"Red\" = 0, \"Blue\" = 1, and \"Green\" = 2.\n",
        "Consideration: This technique is appropriate only when there is an inherent order or ranking among the categories (ordinal data). Using it for nominal data (categories without order) can introduce a false sense of order to the model, which can lead to incorrect interpretations.\n",
        "3.  Target Encoding (Mean Encoding):\n",
        "How it works: This technique replaces each category with the mean of the target variable for that category.\n",
        "Example: If you're predicting house prices and have a \"Neighborhood\" variable, you would replace each neighborhood name with the average house price in that neighborhood.\n",
        "Consideration: This can be effective but is prone to overfitting, especially with small datasets or categories with very few observations. Smoothing techniques or cross-validation are often used to mitigate this.\n",
        "Binary Encoding:\n",
        "How it works: This is a combination of one-hot encoding and label encoding. The categories are first converted to numerical using label encoding, and then these numbers are represented in binary code. Each bit of the binary code creates a new variable.\n",
        "Consideration: This technique is useful when you have a high number of categories, as it reduces the number of new variables compared to one-hot encoding.\n",
        "\n",
        "\n",
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "->In Multiple Linear Regression, interaction terms are used to model situations where the relationship between a dependent variable and one independent variable changes depending on the value of another independent variable.\n",
        "\n",
        "Here's a breakdown of their role:\n",
        "\n",
        "Capturing synergistic or antagonistic effects: Interaction terms allow you to capture effects where the combined influence of two or more independent variables is different from the sum of their individual effects. For example, a drug might be very effective on its own, but its effectiveness could be amplified or reduced when taken with another drug. An interaction term would capture this combined effect.\n",
        "Modeling conditional relationships: They help in understanding how the slope of one independent variable changes as another independent variable changes. This is useful when the relationship between two variables is not constant but is dependent on a third variable.\n",
        "Improving model fit: Including relevant interaction terms can improve the overall fit of the model by accounting for more complex relationships in the data that a purely additive model would miss.\n",
        "Providing more nuanced interpretations: Interaction terms lead to more nuanced interpretations of the regression coefficients. Instead of interpreting the effect of an independent variable in isolation, you interpret its effect in conjunction with the interacting variable.\n",
        "How they are represented:\n",
        "\n",
        "An interaction term is typically created by multiplying two or more independent variables together. For example, if you have independent variables X1 and X2, the interaction term would be X1 * X2. The regression equation would then include this interaction term along with the individual terms:\n",
        "\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "->The interpretation of the intercept can indeed differ between Simple and Multiple Linear Regression due to the presence of multiple independent variables in the latter.\n",
        "\n",
        "Here's a breakdown of the difference:\n",
        "\n",
        "Simple Linear Regression (Y = β₀ + β₁X + ε):\n",
        "\n",
        "In simple linear regression, the intercept (β₀) represents the predicted value of the dependent variable (Y) when the independent variable (X) is zero.\n",
        "Interpretation: This interpretation is straightforward. It's the point where the regression line crosses the Y-axis. However, it's important to consider whether X=0 is a meaningful or realistic value in the context of your data. If X=0 is outside the range of your observed data or doesn't make sense in reality, the intercept might not have a practical interpretation.\n",
        "Multiple Linear Regression (Y = β₀ + β₁X₁ + β₂X₂ + ... + βₚXₚ + ε):\n",
        "\n",
        "In multiple linear regression, the intercept (β₀) represents the predicted value of the dependent variable (Y) when all independent variables (X₁, X₂, ..., Xₚ) are zero.\n",
        "Interpretation: This interpretation is often less straightforward than in simple linear regression. It's the predicted value of Y when all predictors are at their baseline (zero) level.\n",
        "Meaningful vs. Not Meaningful: Similar to simple linear regression, the interpretation of the intercept in multiple linear regression depends on whether the value of zero for all independent variables is meaningful and within the range of the observed data.\n",
        "Extrapolation: If setting all independent variables to zero requires significant extrapolation outside the range of your data, the intercept might not have a practical or reliable interpretation.\n",
        "Dummy Variables: When you have categorical independent variables that have been transformed using techniques like one-hot encoding, the intercept represents the predicted value of the dependent variable when all dummy variables are zero. This typically corresponds to the baseline category that was omitted during the encoding process. In this case, the intercept's interpretation is tied to that specific baseline category.\n",
        "\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "->In regression analysis, the slope (often denoted as 'm' in simple linear regression or 'β₁', 'β₂', etc., in multiple linear regression) is a crucial coefficient that quantifies the estimated change in the dependent variable for a one-unit increase in the corresponding independent variable, while holding other independent variables constant (in the case of multiple regression).\n",
        "\n",
        "Here's a breakdown of its significance and how it affects predictions:\n",
        "\n",
        "Significance of the Slope:\n",
        "\n",
        "Direction and Strength of Relationship: The sign of the slope tells you the direction of the linear relationship between the independent and dependent variables.\n",
        "A positive slope indicates a positive relationship: As the independent variable increases, the dependent variable is predicted to increase.\n",
        "A negative slope indicates a negative relationship: As the independent variable increases, the dependent variable is predicted to decrease.\n",
        "The magnitude of the slope indicates the strength of the relationship. A larger absolute value of the slope suggests a stronger impact of the independent variable on the dependent variable.\n",
        "Quantifying the Impact: The slope provides a quantitative measure of how much the dependent variable is expected to change for each unit change in the independent variable. This is a key insight for understanding the practical significance of the relationship.\n",
        "Statistical Significance: In regression analysis, we also assess the statistical significance of the slope. This is typically done through hypothesis testing (e.g., using a t-test) to determine if the estimated slope is significantly different from zero. A statistically significant slope suggests that there is a real linear relationship between the variables in the population, not just in the sample data.\n",
        "How it Affects Predictions:\n",
        "\n",
        "Determining Predicted Values: The slope is a fundamental component of the regression equation used to make predictions. For simple linear regression (Y = mX + c), the slope 'm' directly influences the predicted value of Y for any given value of X. In multiple linear regression, each slope contributes to the overall predicted value based on the values of their corresponding independent variables.\n",
        "Rate of Change: The slope dictates the rate at which the predicted value of the dependent variable changes as the independent variable changes. A steeper slope means a larger change in the predicted Y for a given change in X.\n",
        "Sensitivity of Predictions: The magnitude of the slope influences how sensitive your predictions are to changes in the independent variable. A larger slope means that small changes in the independent variable will lead to larger changes in the predicted dependent variable.\n",
        "In essence, the slope is the core of the linear relationship modeled by regression. It tells you how the independent variable(s) influence the dependent variable and is directly used to generate predictions based on the model. Understanding the slope is crucial for interpreting the results of a regression analysis and using the model for prediction.In regression analysis, the slope (often denoted as 'm' in simple linear regression or 'β₁', 'β₂', etc., in multiple linear regression) is a crucial coefficient that quantifies the estimated change in the dependent variable for a one-unit increase in the corresponding independent variable, while holding other independent variables constant (in the case of multiple regression).\n",
        "\n",
        "Here's a breakdown of its significance and how it affects predictions:\n",
        "\n",
        "Significance of the Slope:\n",
        "\n",
        "Direction and Strength of Relationship: The sign of the slope tells you the direction of the linear relationship between the independent and dependent variables.\n",
        "A positive slope indicates a positive relationship: As the independent variable increases, the dependent variable is predicted to increase.\n",
        "A negative slope indicates a negative relationship: As the independent variable increases, the dependent variable is predicted to decrease.\n",
        "The magnitude of the slope indicates the strength of the relationship. A larger absolute value of the slope suggests a stronger impact of the independent variable on the dependent variable.\n",
        "Quantifying the Impact: The slope provides a quantitative measure of how much the dependent variable is expected to change for each unit change in the independent variable. This is a key insight for understanding the practical significance of the relationship.\n",
        "Statistical Significance: In regression analysis, we also assess the statistical significance of the slope. This is typically done through hypothesis testing (e.g., using a t-test) to determine if the estimated slope is significantly different from zero. A statistically significant slope suggests that there is a real linear relationship between the variables in the population, not just in the sample data.\n",
        "How it Affects Predictions:\n",
        "\n",
        "Determining Predicted Values: The slope is a fundamental component of the regression equation used to make predictions. For simple linear regression (Y = mX + c), the slope 'm' directly influences the predicted value of Y for any given value of X. In multiple linear regression, each slope contributes to the overall predicted value based on the values of their corresponding independent variables.\n",
        "\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "->The intercept in a regression model provides context for the relationship between variables primarily by establishing a baseline or starting point for the dependent variable when the independent variable(s) are at a specific value (usually zero).\n",
        "\n",
        "Here's how it provides context:\n",
        "\n",
        "Baseline Value: The intercept represents the predicted value of the dependent variable when all independent variables are zero. This provides a baseline from which to interpret the effects of the independent variables. For example, in a model predicting house prices based on square footage, the intercept might represent the predicted price of a house with zero square footage. While this might not be a realistic scenario, it sets the scale for how the price changes as square footage increases.\n",
        "Context with Independent Variable Effects: The intercept gives meaning to the slopes of the independent variables. Each slope tells you the change in the dependent variable for a one-unit increase in the corresponding independent variable, holding other variables constant. This \"holding other variables constant\" is implicitly relative to the baseline established by the intercept.\n",
        "Meaningfulness in Context: The interpretability of the intercept depends heavily on whether the value of zero for the independent variable(s) is meaningful and within the range of the data.\n",
        "Meaningful Zero: If zero is a realistic value for the independent variable (e.g., zero hours of studying, zero degrees Celsius), the intercept has a direct and often intuitive interpretation as the predicted value of the dependent variable when the independent variable is absent or at its starting point.\n",
        "Unrealistic Zero: If zero is outside the range of the data or doesn't make practical sense (e.g., zero height for an adult), the intercept's interpretation is more of a mathematical necessity to define the regression line's position rather than a practically meaningful value. In such cases, the intercept might be more about setting the scale of the relationship than providing a literal prediction at zero.\n",
        "\n",
        "18. What are the limitations of using R2 as a sole measure of model performance?\n",
        "->While R-squared (R²) is a commonly used metric for evaluating the performance of a regression model, it has several limitations when used as the sole measure:\n",
        "\n",
        "R² can be artificially inflated with more predictors: Adding more independent variables to a multiple linear regression model, even irrelevant ones, will almost always increase the R² value. This can give a misleading impression of improved model performance, even if the added variables do not have a true relationship with the dependent variable.\n",
        "R² does not indicate causality: A high R² value indicates a strong linear relationship between the independent variables and the dependent variable, but it does not imply that the independent variables cause the changes in the dependent variable. Correlation does not equal causation.\n",
        "R² does not assess the validity of assumptions: R² does not provide any information about whether the assumptions of linear regression (linearity, independence, homoscedasticity, normality of residuals) are met. A model with a high R² can still violate these assumptions, leading to unreliable inferences.\n",
        "R² does not indicate the quality of predictions: A high R² means the model explains a large proportion of the variance in the dependent variable in the training data. It doesn't guarantee that the model will make accurate predictions on new, unseen data. Overfitting can lead to a high R² on the training data but poor performance on test data.\n",
        "R² does not indicate the significance of individual predictors: A high overall R² does not tell you whether each individual independent variable is statistically significant or contributing meaningfully to the model. You need to look at the p-values or confidence intervals of individual coefficients for that.\n",
        "R² is not suitable for comparing models with different numbers of predictors: Because R² increases with the number of predictors, it's not appropriate to use R² alone to compare models with different sets of independent variables. Adjusted R² is a better metric for this purpose as it penalizes the inclusion of unnecessary predictors.\n",
        "R² doesn't tell you about the direction of the relationship: R² is a measure of the strength of the linear association, but it doesn't tell you whether the relationship is positive or negative. You need to look at the signs of the coefficients for that.\n",
        "\n",
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "->Interpreting a large standard error for a regression coefficient is important for understanding the reliability and precision of your model's estimates. Here's what a large standard error generally indicates:\n",
        "\n",
        "Low Precision of the Estimate: A large standard error means that the estimated coefficient for that independent variable is less precise. The true value of the coefficient in the population is likely to be further away from the estimated value in your sample. Think of the standard error as a measure of the uncertainty around your estimated coefficient. A larger standard error implies more uncertainty.\n",
        "Less Reliable Coefficient: With a large standard error, the estimated coefficient is less reliable. If you were to take different samples from the same population, the estimated coefficient for that variable would likely vary more widely from sample to sample compared to a variable with a small standard error.\n",
        "Wider Confidence Interval: The confidence interval for the coefficient is calculated using the standard error. A large standard error will result in a wider confidence interval. A wide confidence interval suggests that you are less certain about the true value of the coefficient. For example, a confidence interval of [ -5, 10 ] for a coefficient is much less informative than an interval of [ 1, 2 ].\n",
        "Reduced Statistical Significance: A large standard error makes it less likely that the coefficient will be statistically significant. The t-statistic (used to test the hypothesis that the coefficient is zero) is calculated by dividing the estimated coefficient by its standard error. A larger standard error will result in a smaller t-statistic (in absolute value), making it harder to reject the null hypothesis that the true coefficient is zero. This means you have less evidence to conclude that the independent variable has a real, non-zero effect on the dependent variable.\n",
        "\n",
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "->You're asking about a key assumption in regression analysis! Heteroscedasticity, or unequal variance of residuals, is important to identify because it impacts the reliability of your model's inferences.\n",
        "\n",
        "How to Identify Heteroscedasticity in Residual Plots:\n",
        "\n",
        "Residual plots are scatter plots where the x-axis represents the predicted values of the dependent variable (or sometimes the independent variable) and the y-axis represents the residuals (the differences between the observed and predicted values). When looking at a residual plot, here's what to look for to identify heteroscedasticity:\n",
        "\n",
        "Cone or Fan Shape: The most common indicator of heteroscedasticity is a cone or fan shape in the residual plot. This means that the spread of the residuals increases or decreases as the predicted values (or independent variable values) increase. The residuals are clustered tightly at one end of the plot and spread out at the other.\n",
        "Unequal Bands: Instead of a clear cone, you might see that the band of residuals around the zero line is not of equal width across the range of predicted values. It might be wider in some sections and narrower in others.\n",
        "Patterns: Any systematic pattern in the spread of the residuals as the predicted values change suggests heteroscedasticity. Ideally, the residuals should be randomly scattered around the zero line with no discernible pattern.\n",
        "Why is it Important to Address Heteroscedasticity?\n",
        "\n",
        "Addressing heteroscedasticity is crucial because its presence violates one of the key assumptions of ordinary least squares (OLS) regression. This violation has significant consequences for the reliability of your model's results:\n",
        "\n",
        "Incorrect Standard Errors: The most critical impact is on the standard errors of the regression coefficients. In the presence of heteroscedasticity, the standard errors are biased. They are often underestimated, which makes your statistical tests (like t-tests for individual coefficients) and confidence intervals unreliable.You're asking about a key assumption in regression analysis! Heteroscedasticity, or unequal variance of residuals, is important to identify because it impacts the reliability of your model's inferences.\n",
        "Why is it Important to Address Heteroscedasticity?\n",
        "\n",
        "Addressing heteroscedasticity is crucial because its presence violates one of the key assumptions of ordinary least squares (OLS) regression. This violation has significant consequences for the reliability of your model's results:\n",
        "\n",
        "Incorrect Standard Errors: The most critical impact is on the standard errors of the regression coefficients. In the presence of heteroscedasticity, the standard errors are biased. They are often underestimated, which makes your statistical tests (like t-tests for individual coefficients) and confidence intervals unreliable.\n",
        "\n",
        "21. What does it mean if a Multiple Linear Regression model has a high R2 but low adjusted R2?\n",
        "->That's a great observation and points to an important concept in evaluating multiple linear regression models!\n",
        "\n",
        "Here's what it means if a Multiple Linear Regression model has a high R² but a low adjusted R²:\n",
        "\n",
        "R²: R² (R-squared) measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s). In a multiple linear regression, it tells you how well all the independent variables, together, explain the variation in the dependent variable. A high R² suggests that the model explains a large portion of the dependent variable's variability.\n",
        "Adjusted R²: Adjusted R² is a modified version of R² that has been adjusted for the number of predictors in the model. It penalizes the inclusion of unnecessary independent variables. Unlike R², adjusted R² only increases if the new term improves the model more than would be expected by chance.\n",
        "The Meaning of High R² and Low Adjusted R²:\n",
        "\n",
        "When you have a high R² but a significantly lower adjusted R², it indicates that your model likely includes independent variables that are not contributing meaningfully to explaining the variance in the dependent variable.\n",
        "\n",
        "High R²: The model with all the independent variables appears to explain a good amount of the variation in the dependent variable.\n",
        "Low Adjusted R²: When you account for the number of independent variables used, the model's explanatory power is not as strong as the R² value alone suggests. This is because some of the variables are likely adding complexity without providing a proportional increase in the model's ability to explain the outcome.\n",
        "In essence, a large gap between R² and adjusted R² is a red flag for overfitting or including irrelevant predictors. Adding more predictors, even random noise, will always increase R². Adjusted R² helps to correct for this by lowering its value when predictors are added that don't improve the model's fit sufficiently.\n",
        "\n",
        "What to do:\n",
        "\n",
        "If you see this pattern, it's a good indication that you should investigate which independent variables are not statistically significant (look at their p-values) and consider removing them from the model. A simpler model with a lower R² but a higher adjusted R² is generally preferred because it's less likely to be overfit and more likely to generalize well to new data. Adjusted R² helps you find the right balance between model complexity and explanatory power.\n",
        "\n",
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "->Scaling variables in Multiple Linear Regression is important for several reasons, although it's not always strictly necessary depending on the specific regression technique you're using. Here's why it's often recommended:\n",
        "\n",
        "Gradient Descent Convergence: Many optimization algorithms used to fit linear regression models (like Gradient Descent, which is common in machine learning frameworks) converge much faster when the independent variables are on similar scales. If variables have vastly different ranges, the cost function will have an elongated shape, making it harder for the algorithm to find the minimum efficiently.\n",
        "Regularization Techniques: Techniques like Ridge and Lasso regression (L1 and L2 regularization) are sensitive to the scale of the independent variables. These techniques add a penalty term to the cost function based on the magnitude of the coefficients. If variables are not scaled, variables with larger scales will have larger coefficients (assuming similar impact on the dependent variable), and the regularization penalty will disproportionately affect them. Scaling ensures that the regularization penalizes coefficients based on their actual impact rather than their scale.\n",
        "Interpretation of Coefficients (with caveats): While scaling doesn't change the fundamental relationship between the variables or the model's predictions, it can sometimes make the interpretation of coefficients easier in specific contexts. When variables are scaled (e.g., standardized to have a mean of 0 and standard deviation of 1), the coefficients represent the change in the dependent variable for a one-standard deviation increase in the independent variable. This can be useful for comparing the relative importance of different independent variables, although other methods like examining standardized coefficients directly might be more appropriate.\n",
        "\n",
        "23. What is polynomial regression?\n",
        "->Polynomial regression is a form of regression analysis in which the relationship between the independent variable (x) and the dependent variable (y) is modeled as an nth degree polynomial. While it models a non-linear relationship between x and y, it is still considered a form of linear regression because the model is linear in the coefficients.\n",
        "\n",
        "24. How does polynomial regression differ from linear regression?\n",
        "->The main difference between polynomial regression and simple linear regression lies in the functional form of the relationship they model between the independent and dependent variables.\n",
        "\n",
        "Here's a breakdown of the key differences:\n",
        "\n",
        "Nature of the Relationship Modeled:\n",
        "Simple Linear Regression: Models a strictly linear relationship between the independent variable (X) and the dependent variable (Y). The relationship is represented by a straight line. This assumes that a one-unit change in X always results in a constant change in Y. $Y = \\beta_0 + \\beta_1 X + \\epsilon$$Y = \\beta_0 + \\beta_1 X + \\epsilon$\n",
        "Polynomial Regression: Models a non-linear relationship between the independent variable (X) and the dependent variable (Y) by including polynomial terms (like $X^2$$X^2$, $X^3$$X^3$, etc.) in the model. This allows the model to fit curves to the data, capturing relationships where the rate of change in Y with respect to X is not constant. $Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + ... + \\beta_n X^n + \\epsilon$$Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + ... + \\beta_n X^n + \\epsilon$\n",
        "Equation Structure:\n",
        "Simple Linear Regression: Has a linear equation with only the independent variable raised to the power of 1.\n",
        "Polynomial Regression: Includes terms where the independent variable is raised to powers greater than 1.\n",
        "\n",
        "25. When is polynomial regression used?\n",
        "->Polynomial regression is used when the relationship between the independent variable and the dependent variable is not linear, but instead exhibits a curved pattern. Here are some common scenarios and reasons for using it:\n",
        "\n",
        "Capturing Non-Linear Relationships: This is the primary reason. If a scatter plot of your data shows a curve rather than a straight line, polynomial regression can provide a better fit than simple linear regression. For example, the relationship between temperature and the efficiency of a process might be curved, or the relationship between advertising spending and sales might show diminishing returns.\n",
        "Improving Model Fit: When a linear model doesn't adequately capture the trend in the data (indicated by patterns in residual plots), adding polynomial terms can improve the model's ability to explain the variance in the dependent variable and reduce the systematic errors (residuals).\n",
        "\n",
        "26. What is the general equation for polynomial regression?\n",
        "->The general equation for polynomial regression models the relationship between a dependent variable ($Y$$Y$) and an independent variable ($X$$X$) as an nth degree polynomial.\n",
        "\n",
        "The equation is typically written as:\n",
        "\n",
        "$Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + ... + \\beta_n X^n + \\epsilon$$Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + ... + \\beta_n X^n + \\epsilon$\n",
        "\n",
        "Where:\n",
        "\n",
        "$Y$$Y$: The dependent variable (the variable you are trying to predict).\n",
        "$X$$X$: The independent variable (the variable you are using to make predictions).\n",
        "$\\beta_0$$\\beta_0$: The intercept (the value of $Y$$Y$ when $X$$X$ is zero).\n",
        "$\\beta_1, \\beta_2, ..., \\beta_n$$\\beta_1, \\beta_2, ..., \\beta_n$: The coefficients for the polynomial terms. These are the values that the regression model estimates from the data.\n",
        "$X^2, ..., X^n$$X^2, ..., X^n$: The polynomial terms, which are the independent variable raised to different powers.\n",
        "$n$$n$: The degree of the polynomial. This determines the complexity of the curve that the model can fit. For example, if $n=2$$n=2$, it's a quadratic polynomial ($Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\epsilon$$Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\epsilon$), and if $n=3$$n=3$, it's a cubic polynomial ($Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\epsilon$$Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\epsilon$).\n",
        "$\\epsilon$$\\epsilon$: The error term (represents the random error or the part of $Y$$Y$ that cannot be explained by the model).\n",
        "This equation shows that polynomial regression is essentially an extension of multiple linear regression, where the independent variables are simply powers of the original independent variable. Despite the non-linear relationship between $Y$$Y$ and $X$$X$, the model is linear in the coefficients ($\\beta$$\\beta$ values), which is why it's still categorized under the umbrella of linear models.\n",
        "\n",
        "\n",
        "27. Can polynomial regression be applied to multiple variables?\n",
        "->Yes, absolutely! Polynomial regression can definitely be applied to multiple independent variables. This is known as multivariate polynomial regression.\n",
        "\n",
        "When you have multiple independent variables, you can include polynomial terms for each of them, as well as interaction terms between the polynomial terms of different variables.\n",
        "\n",
        "For example, with two independent variables, $X_1$$X_1$ and $X_2$$X_2$, a second-degree polynomial regression model could look like this:\n",
        "\n",
        "$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1^2 + \\beta_4 X_2^2 + \\beta_5 X_1 X_2 + \\epsilon$$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1^2 + \\beta_4 X_2^2 + \\beta_5 X_1 X_2 + \\epsilon$\n",
        "\n",
        "As you can see, with multiple variables and higher degrees, the number of terms in the equation can grow quite large, including all combinations of the variables raised to different powers up to the specified degree, as well as interaction terms between these polynomial terms.\n",
        "\n",
        "28. What are the limitations of polynomial regression?\n",
        "->Polynomial regression is a flexible tool for modeling non-linear relationships, but it does come with its own set of limitations. Here are some of the key drawbacks:\n",
        "\n",
        "Overfitting: This is perhaps the most significant limitation. Using a high-degree polynomial can lead to a model that fits the training data extremely well but fails to generalize to new, unseen data. The model essentially memorizes the training data's noise and quirks rather than capturing the underlying relationship. This is especially true with limited data points.\n",
        "Choosing the Right Degree: Selecting the appropriate degree of the polynomial is often more of an art than a science. There's no definitive rule, and choosing too low or too high a degree can result in a poor model. This often requires experimentation and using model evaluation techniques, which adds complexity to the modeling process.\n",
        "Extrapolation Risks: Polynomial models can behave erratically outside the range of the data they were trained on. Extrapolating predictions with a polynomial model can lead to highly unreliable and nonsensical results, as the curve may bend in unexpected ways beyond the observed data.\n",
        "\n",
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "->You've hit on a crucial point when using polynomial regression – selecting the right degree is key to avoiding overfitting and finding a model that generalizes well. Here are some common methods used to evaluate model fit and help select the appropriate degree of a polynomial:\n",
        "\n",
        "Visual Inspection of Scatter and Residual Plots:\n",
        "Scatter Plot: Start by plotting the dependent variable against the independent variable. This can give you a visual sense of the curvature in the data and suggest a potential degree.\n",
        "Residual Plot: After fitting a polynomial model of a certain degree, plot the residuals against the predicted values or the independent variable. Look for patterns in the residuals. If a lower-degree polynomial is insufficient, you'll often see a systematic pattern (e.g., a curve) in the residual plot. As you increase the degree and the model fits the non-linear trend better, the residuals should become more randomly scattered around zero.\n",
        "R-squared and Adjusted R-squared:\n",
        "R-squared: As you increase the degree of the polynomial, R-squared will generally increase or stay the same, as adding more terms will always improve or maintain the fit to the training data. However, a high R-squared alone doesn't guarantee a good model due to the risk of overfitting.\n",
        "Adjusted R-squared: Adjusted R-squared is more useful here because it penalizes the inclusion of extra terms. You can fit models with different degrees and compare their adjusted R-squared values. A higher adjusted R-squared suggests a better model, taking into account the number of predictors (polynomial terms). Look for the point where adding a higher-degree term no longer significantly increases the adjusted R-squared.\n",
        "\n",
        "30. Why is visualization important in polynomial regression?\n",
        "->Identifying Non-Linear Relationships: The first step in deciding whether to use polynomial regression is often visualizing the data. A scatter plot of the dependent variable against the independent variable can reveal if the relationship is curved rather than linear, suggesting that a polynomial model might be appropriate.\n",
        "Selecting the Appropriate Degree: Visualizing the data and comparing the fit of polynomial models with different degrees on a scatter plot can help you get a sense of which degree best captures the curvature without appearing to overfit the noise in the data.\n",
        "Assessing Model Fit: After fitting a polynomial model, visualizing the fitted curve overlaid on the scatter plot allows you to visually assess how well the model fits the data points. Does the curve follow the general trend? Does it seem to capture the non-linear pattern?\n",
        "Detecting Overfitting: Visualizing the fitted curve is particularly useful for detecting overfitting. If you fit a very high-degree polynomial, the curve might wiggle excessively to pass through every data point, indicating that it's fitting the noise rather than the underlying pattern. This becomes visually apparent on a scatter plot.\n",
        "\n",
        "31. How is polynomial regression implemented in Python?\n",
        "->"
      ],
      "metadata": {
        "id": "UGuKpTJIxfDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([1.2, 1.9, 3.2, 4.8, 6.1])\n",
        "\n",
        "# Fit a polynomial of degree 2\n",
        "coefficients = np.polyfit(x, y, 2)\n",
        "polynomial = np.poly1d(coefficients)\n",
        "\n",
        "# Generate predictions\n",
        "x_pred = np.linspace(min(x), max(x), 100)\n",
        "y_pred = polynomial(x_pred)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(x, y, color='blue', label='Data')\n",
        "plt.plot(x_pred, y_pred, color='red', label='Polynomial Fit')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "c6dEXielLpdn",
        "outputId": "6e031d76-0456-470b-ed4e-20fc15e6c87f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}